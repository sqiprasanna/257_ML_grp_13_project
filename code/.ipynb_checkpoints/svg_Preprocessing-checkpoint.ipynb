{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae40314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e70f7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkWord(token):\n",
    "    negations = ['not', 'no']\n",
    "    fillers = ['link', 'http']\n",
    "    return (not token.is_stop) and (token.is_alpha) and (token.text not in fillers) and ((token.text in negations) or (len(token.text) > 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c282cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkToken(token):\n",
    "    fillers = set({'link', 'http'})\n",
    "    common = set() #set({'global', 'warming', 'climate', 'change'}) # Remove common signal between tweets\n",
    "    return (len(token.text) > 3) and (not token.is_stop) and (token.is_alpha) and (token.text.lower() not in fillers) and (token.text.lower() not in common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a0ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWords(t):\n",
    "    splitTweet = []\n",
    "    t = t.split(' ')\n",
    "    \n",
    "    for word in t:\n",
    "        res = re.search(r'[A-Z]', word)\n",
    "        if res is not None:\n",
    "            ch = word[res.span()[0]]\n",
    "            words = re.split(r'[A-Z]', word)\n",
    "            splitTweet.extend([words[0], ch + words[1]])\n",
    "        else:\n",
    "            splitTweet.extend(re.split(r'[\"|,;!|:*]', word))\n",
    "    \n",
    "    return ' '.join(splitTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "923658f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacyPipeline(tweets, verbose=False):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    enchant_dict = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    MIN_TWEET_LEN = 4\n",
    "    \n",
    "    indices = []\n",
    "    preprocessed_tweets = []\n",
    "    for index, t in enumerate(tweets):\n",
    "           \n",
    "        if verbose:\n",
    "            print(t)\n",
    "        \n",
    "        # Tokenizing tweet with spaCy\n",
    "        doc = nlp(t)\n",
    "        filtered_tweet = set()\n",
    "        \n",
    "        # Finding country or city names\n",
    "        locs = set()\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "                for word in ent.text.lower().split(' '):\n",
    "                    locs.add(word)\n",
    "                        \n",
    "        if verbose:\n",
    "            print(\"locs: \", locs)\n",
    "        \n",
    "        # Filter through words \n",
    "        for token in doc:\n",
    "            # Check for words not in english dictionary\n",
    "            if not enchant_dict.check((str(token.text.lower()))): \n",
    "                continue\n",
    "                \n",
    "            # Check for duplicate words\n",
    "            if token.lemma_ in filtered_tweet:\n",
    "                continue\n",
    "                \n",
    "            # print(token, \" | \", spacy.explain(token.pos_))\n",
    "            \n",
    "            if (token.text.lower() not in locs) and checkToken(token):\n",
    "                    filtered_tweet.add(token.lemma_.lower())\n",
    "        \n",
    "        if verbose:\n",
    "            print(filtered_tweet, \"\\n---\\n\")\n",
    "        \n",
    "        # Filter out tweets that have too few words\n",
    "        if len(filtered_tweet) >= MIN_TWEET_LEN:\n",
    "            preprocessed_tweets.append(filtered_tweet)\n",
    "            indices.append(index)\n",
    "    \n",
    "    return preprocessed_tweets, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9283c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweets,verbose):\n",
    "\n",
    "    whiteList=['climate','change','earth','global','warming','planet']\n",
    "\n",
    "    #Remove mentions\n",
    "    count=0\n",
    "    mentions_tweets = [re.findall('@\\w+', tweet) for tweet in tweets]\n",
    "\n",
    "    #Remove hash sign\n",
    "    hash_tweets = [re.findall('#\\w+', tweet) for tweet in tweets]\n",
    "\n",
    "    #Remove urls\n",
    "    urls_tweets = [re.findall(r'http.?://[^\\s]+[\\s]?', tweet) for tweet in tweets]\n",
    "\n",
    "\n",
    "    #Convert all to lowercase\n",
    "    tweets = [t.lower() for t in tweets]\n",
    "    \n",
    "    # Process tweets through spaCy pipeline\n",
    "    tweets, indices = spacyPipeline(tweets,False)\n",
    "    \n",
    "    # print(tweets)\n",
    "    return tweets,hash_tweets,urls_tweets,mentions_tweets,indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
